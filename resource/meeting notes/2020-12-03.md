- the FAT20 paper s high level about what can be done, but not how to do it

- how to check weather if the internal actually works or not (attention mechnism)

- schema of the data set

---

if for ML model development, you know what's intention, but in model card, it not cover untanglement
for the dark colour people, even having 50%, still have the high chance get error, it is because dark people with more decortion, more diverse
once untanglement happens, need attention mech, or interpreattion to reflect that to audit

---

huge gap between audit process and real case
where audit team can pass the model, but the result can be still incorrect because of the unexpected entanglement
to fix the gap, need a relection for the unexpected entanglement, and attention mech, etc are ways to reveal the unexpected entanglement
add something transparencey that will help audit, and some cases can be fixed in the internal process, before deployment

the texture data could be an evidence, or some other evidence, that even with model card and data sheet, cannot guanrantee the quality

---

try to come out some example with code

---

try to read more paper from the introduction of the previous paper, as they have related work in the section

---

set up a overleaf page, and put all paper references into it

---

probably setup a github page as well
