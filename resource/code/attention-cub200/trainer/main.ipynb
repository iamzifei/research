{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMqZqEfGfFHucO9NXGhQAnx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"922bf2c0fd294baf9b10122f33bf2a0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c4d5daaad4dd4be88b6f5429dc167076","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aabd2ebe8d6a48f4ad3a8034d293174b","IPY_MODEL_1ab935400fc447e09ddc12d2e1cf0429"]}},"c4d5daaad4dd4be88b6f5429dc167076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aabd2ebe8d6a48f4ad3a8034d293174b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_605798c3cbc34d8fafa2685e4314220e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f97902119c6743aab8b9a7fc5e0e81ec"}},"1ab935400fc447e09ddc12d2e1cf0429":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1077a6fc8d1747ffb5419c8c46fd7798","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 73728/? [00:18&lt;00:00, 549225.26it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_120317ba402f4248a9a692b25bea9111"}},"605798c3cbc34d8fafa2685e4314220e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f97902119c6743aab8b9a7fc5e0e81ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1077a6fc8d1747ffb5419c8c46fd7798":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"120317ba402f4248a9a692b25bea9111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"SuK9YDKiz0Sp","executionInfo":{"status":"ok","timestamp":1601872639018,"user_tz":-660,"elapsed":59702,"user":{"displayName":"Zifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd3Z2Ytor1g1CxAsN0fDtxrvdCVdGiX-mKjy_nRkI=s64","userId":"13946056825620422407"}},"outputId":"5e7db79b-707d-48b9-9d37-da31b9e1251a","colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# 'cs231n' folder containing the '.py', 'classifiers' and 'datasets'\n","# folders.\n","# e.g. 'cs231n/assignments/assignment1/cs231n/'\n","FOLDERNAME = 'LearnToPayAttention'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","%cd drive/My\\ Drive\n","%cp -r $FOLDERNAME ../../\n","%cd ../../$FOLDERNAME\n","!pip install tensorboardX Pillow"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive\n","/content/LearnToPayAttention\n","Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fDe8KzYo0KBt","executionInfo":{"status":"ok","timestamp":1601872683044,"user_tz":-660,"elapsed":2241,"user":{"displayName":"Zifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd3Z2Ytor1g1CxAsN0fDtxrvdCVdGiX-mKjy_nRkI=s64","userId":"13946056825620422407"}}},"source":["# This is a bit of magic to make matplotlib figures appear inline in the notebook\n","# rather than in a new window.\n","%matplotlib inline\n","\n","# Some more magic so that the notebook will reload external python modules;\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yJokX5aLsp5","executionInfo":{"status":"error","timestamp":1601874929528,"user_tz":-660,"elapsed":1387,"user":{"displayName":"Zifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd3Z2Ytor1g1CxAsN0fDtxrvdCVdGiX-mKjy_nRkI=s64","userId":"13946056825620422407"}},"outputId":"1e042588-92c5-41fc-a20a-80d20907d45d","colab":{"base_uri":"https://localhost:8080/","height":368}},"source":["from dataset import CUB\n","import transforms\n","from torch.utils.data import DataLoader\n","import time\n","\n","time1 = time.time()\n","IMAGE_SIZE = 448\n","TRAIN_MEAN = [0.48560741861744905, 0.49941626449353244, 0.43237713785804116]\n","TRAIN_STD = [0.2321024260764962, 0.22770540015765814, 0.2665100547329813]\n","TEST_MEAN = [0.4862169586881995, 0.4998156522834164, 0.4311430419332438]\n","TEST_STD = [0.23264268069040475, 0.22781080253662814, 0.26667253517177186]\n","\n","path = 'CUB_200_2011'\n","\n","train_transforms = transforms.Compose([\n","        transforms.ToCVImage(),\n","        transforms.RandomResizedCrop(IMAGE_SIZE),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(TRAIN_MEAN, TRAIN_STD)\n","    ])\n","\n","test_transforms = transforms.Compose([\n","    transforms.ToCVImage(),\n","    transforms.Resize(IMAGE_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(TEST_MEAN,TEST_STD)\n","    ])\n","\n","train_dataset = CUB(\n","        path,\n","        train=True,\n","        transform=train_transforms,\n","        target_transform=None\n","    )\n","    # print(len(train_dataset))\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=16,\n","    num_workers=0,\n","    shuffle=True\n",")\n","\n","test_dataset = CUB(\n","        path,\n","        train=False,\n","        transform=test_transforms,\n","        target_transform=None\n","    )\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=16,\n","    num_workers=0,\n","    shuffle=True\n",")\n","\n","time2 = time.time()\n","\n","print(time2-time1)\n","print(len(train_dataloader))\n","print(len(test_dataloader))\n","\n"],"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-198188886e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# print(len(train_dataset))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/LearnToPayAttention/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, train, transform, target_transform)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CUB_200_2011/images.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"togPdkZK0keE"},"source":["# Training\n","1. Pay attention before max-pooling layers\n","`python train.py --attn_mode before --outf logs_before --normalize_attn --log_images`\n","2. Pay attention after max-pooling layers\n","`python train.py --attn_mode after --outf logs_after --normalize_attn --log_images`"]},{"cell_type":"code","metadata":{"id":"zoSvR2uN0TmL","executionInfo":{"status":"error","timestamp":1601874773569,"user_tz":-660,"elapsed":2793,"user":{"displayName":"Zifei Gong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggd3Z2Ytor1g1CxAsN0fDtxrvdCVdGiX-mKjy_nRkI=s64","userId":"13946056825620422407"}},"outputId":"79205c1d-af37-4286-9d11-c3a908d6e3e8","colab":{"base_uri":"https://localhost:8080/","height":478,"referenced_widgets":["922bf2c0fd294baf9b10122f33bf2a0e","c4d5daaad4dd4be88b6f5429dc167076","aabd2ebe8d6a48f4ad3a8034d293174b","1ab935400fc447e09ddc12d2e1cf0429","605798c3cbc34d8fafa2685e4314220e","f97902119c6743aab8b9a7fc5e0e81ec","1077a6fc8d1747ffb5419c8c46fd7798","120317ba402f4248a9a692b25bea9111"]}},"source":["import os\n","import random\n","import argparse\n","import numpy as np\n","from tensorboardX import SummaryWriter\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torchvision\n","import torchvision.utils as utils\n","import torchvision.transforms as transforms\n","from model1 import AttnVGG_before\n","from model2 import AttnVGG_after\n","from utilities import *\n","from cub200 import CUB\n","\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","\n","# parser = argparse.ArgumentParser(description=\"LearnToPayAttn-CIFAR100\")\n","\n","# parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size\")\n","# parser.add_argument(\"--epochs\", type=int, default=300, help=\"number of epochs\")\n","# parser.add_argument(\"--lr\", type=float, default=0.1, help=\"initial learning rate\")\n","# parser.add_argument(\"--outf\", type=str, default=\"logs\", help='path of log files')\n","# parser.add_argument(\"--attn_mode\", type=str, default=\"after\", help='insert attention modules before OR after maxpooling layers')\n","\n","# parser.add_argument(\"--normalize_attn\", action='store_true', help='if True, attention map is normalized by softmax; otherwise use sigmoid')\n","# parser.add_argument(\"--no_attention\", action='store_true', help='turn down attention')\n","# parser.add_argument(\"--log_images\", action='store_true', help='log images and (is available) attention maps')\n","\n","# opt = parser.parse_args()\n","\n","class Args:\n","  batch_size = 128\n","  epochs = 10\n","  lr = 0.1\n","  outf=\"logs\"\n","  attn_mode=\"after\"\n","  normalize_attn=True\n","  no_attention=False\n","  log_images=True\n","  gpu=True\n","\n","opt = Args()\n","\n","\n","def main():\n","    ## load data\n","    # CIFAR-100: 500 training images and 100 testing images per class\n","    print('\\nloading the dataset ...\\n')\n","    num_aug = 3\n","    im_size = 224\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(im_size, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","    ])\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","    ])\n","    trainset = Cub2011(root='CUB200_data', train=True, transform=transform_train)\n","    testset = Cub2011(root='CUB200_data', train=False, transform=transform_test)\n","    # trainset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=True, download=True, transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(cub200_data, batch_size=opt.batch_size, shuffle=True, num_workers=8)\n","    # testset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=False, download=True, transform=transform_test)\n","    testloader = torch.utils.data.DataLoader(cub200_data, batch_size=100, shuffle=False, num_workers=5)\n","    print('done')\n","\n","    ## load network\n","    print('\\nloading the network ...\\n')\n","    # use attention module?\n","    if not opt.no_attention:\n","        print('\\nturn on attention ...\\n')\n","    else:\n","        print('\\nturn off attention ...\\n')\n","    # (linear attn) insert attention befroe or after maxpooling?\n","    # (grid attn only supports \"before\" mode)\n","    if opt.attn_mode == 'before':\n","        print('\\npay attention before maxpooling layers...\\n')\n","        net = AttnVGG_before(im_size=im_size, num_classes=100,\n","            attention=not opt.no_attention, normalize_attn=opt.normalize_attn, init='xavierUniform')\n","    elif opt.attn_mode == 'after':\n","        print('\\npay attention after maxpooling layers...\\n')\n","        net = AttnVGG_after(im_size=im_size, num_classes=100,\n","            attention=not opt.no_attention, normalize_attn=opt.normalize_attn, init='xavierUniform')\n","    else:\n","        raise NotImplementedError(\"Invalid attention mode!\")\n","    criterion = nn.CrossEntropyLoss()\n","    print('done')\n","\n","    ## move to GPU\n","    print('\\nmoving to GPU ...\\n')\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    # model = net.cuda()\n","    # criterion = criterion.cuda()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  # device_ids = [0,1]\n","    model = nn.DataParallel(net).to(device)\n","    criterion.to(device)\n","    print('done')\n","\n","    ### optimizer\n","    optimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=0.9, weight_decay=5e-4)\n","    lr_lambda = lambda epoch : np.power(0.5, int(epoch/25))\n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n","\n","    # training\n","    print('\\nstart training ...\\n')\n","    step = 0\n","    running_avg_accuracy = 0\n","    writer = SummaryWriter(opt.outf)\n","    for epoch in range(opt.epochs):\n","        images_disp = []\n","        # adjust learning rate\n","        scheduler.step()\n","        writer.add_scalar('train/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n","        print(\"\\nepoch %d learning rate %f\\n\" % (epoch, optimizer.param_groups[0]['lr']))\n","        # run for one epoch\n","        for aug in range(num_aug):\n","            for i, data in enumerate(trainloader, 0):\n","                # warm up\n","                model.train()\n","                model.zero_grad()\n","                optimizer.zero_grad()\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                if (aug == 0) and (i == 0): # archive images in order to save to logs\n","                    images_disp.append(inputs[0:36,:,:,:])\n","                # forward\n","                pred, __, __, __ = model(inputs)\n","                # backward\n","                loss = criterion(pred, labels)\n","                loss.backward()\n","                optimizer.step()\n","                # display results\n","                if i % 10 == 0:\n","                    model.eval()\n","                    pred, __, __, __ = model(inputs)\n","                    predict = torch.argmax(pred, 1)\n","                    total = labels.size(0)\n","                    correct = torch.eq(predict, labels).sum().double().item()\n","                    accuracy = correct / total\n","                    running_avg_accuracy = 0.9*running_avg_accuracy + 0.1*accuracy\n","                    writer.add_scalar('train/loss', loss.item(), step)\n","                    writer.add_scalar('train/accuracy', accuracy, step)\n","                    writer.add_scalar('train/running_avg_accuracy', running_avg_accuracy, step)\n","                    print(\"[epoch %d][aug %d/%d][%d/%d] loss %.4f accuracy %.2f%% running avg accuracy %.2f%%\"\n","                        % (epoch, aug, num_aug-1, i, len(trainloader)-1, loss.item(), (100*accuracy), (100*running_avg_accuracy)))\n","                step += 1\n","        # the end of each epoch: test & log\n","        print('\\none epoch done, saving records ...\\n')\n","        torch.save(model.state_dict(), os.path.join(opt.outf, 'net.pth'))\n","        if epoch == opt.epochs / 2:\n","            torch.save(model.state_dict(), os.path.join(opt.outf, 'net%d.pth' % epoch))\n","        model.eval()\n","        total = 0\n","        correct = 0\n","        with torch.no_grad():\n","            # log scalars\n","            for i, data in enumerate(testloader, 0):\n","                images_test, labels_test = data\n","                images_test, labels_test = images_test.to(device), labels_test.to(device)\n","                if i == 0: # archive images in order to save to logs\n","                    images_disp.append(inputs[0:36,:,:,:])\n","                pred_test, __, __, __ = model(images_test)\n","                predict = torch.argmax(pred_test, 1)\n","                total += labels_test.size(0)\n","                correct += torch.eq(predict, labels_test).sum().double().item()\n","            writer.add_scalar('test/accuracy', correct/total, epoch)\n","            print(\"\\n[epoch %d] accuracy on test data: %.2f%%\\n\" % (epoch, 100*correct/total))\n","            # log images\n","            if opt.log_images:\n","                print('\\nlog images ...\\n')\n","                I_train = utils.make_grid(images_disp[0], nrow=6, normalize=True, scale_each=True)\n","                writer.add_image('train/image', I_train, epoch)\n","                if epoch == 0:\n","                    I_test = utils.make_grid(images_disp[1], nrow=6, normalize=True, scale_each=True)\n","                    writer.add_image('test/image', I_test, epoch)\n","            if opt.log_images and (not opt.no_attention):\n","                print('\\nlog attention maps ...\\n')\n","                # base factor\n","                if opt.attn_mode == 'before':\n","                    min_up_factor = 1\n","                else:\n","                    min_up_factor = 2\n","                # sigmoid or softmax\n","                if opt.normalize_attn:\n","                    vis_fun = visualize_attn_softmax\n","                else:\n","                    vis_fun = visualize_attn_sigmoid\n","                # training data\n","                __, c1, c2, c3 = model(images_disp[0])\n","                if c1 is not None:\n","                    attn1 = vis_fun(I_train, c1, up_factor=min_up_factor, nrow=6)\n","                    writer.add_image('train/attention_map_1', attn1, epoch)\n","                if c2 is not None:\n","                    attn2 = vis_fun(I_train, c2, up_factor=min_up_factor*2, nrow=6)\n","                    writer.add_image('train/attention_map_2', attn2, epoch)\n","                if c3 is not None:\n","                    attn3 = vis_fun(I_train, c3, up_factor=min_up_factor*4, nrow=6)\n","                    writer.add_image('train/attention_map_3', attn3, epoch)\n","                # test data\n","                __, c1, c2, c3 = model(images_disp[1])\n","                if c1 is not None:\n","                    attn1 = vis_fun(I_test, c1, up_factor=min_up_factor, nrow=6)\n","                    writer.add_image('test/attention_map_1', attn1, epoch)\n","                if c2 is not None:\n","                    attn2 = vis_fun(I_test, c2, up_factor=min_up_factor*2, nrow=6)\n","                    writer.add_image('test/attention_map_2', attn2, epoch)\n","                if c3 is not None:\n","                    attn3 = vis_fun(I_test, c3, up_factor=min_up_factor*4, nrow=6)\n","                    writer.add_image('test/attention_map_3', attn3, epoch)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","loading the dataset ...\n","\n","Downloading http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz to CUB200_data/CUB_200_2011.tgz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"922bf2c0fd294baf9b10122f33bf2a0e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-df87cc8cb20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-df87cc8cb20a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5071\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4867\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4408\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.2675\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2565\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2761\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     ])\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCub2011\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CUB200_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCub2011\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CUB200_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# trainset = torchvision.datasets.CIFAR100(root='CIFAR100_data', train=True, download=True, transform=transform_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/LearnToPayAttention/cub200.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, loader, download)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/LearnToPayAttention/cub200.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# check integrity of downloaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File not found or corrupted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: File not found or corrupted."]}]}]}